# demand-pricing-forecasting
#  Intelligent Demand Forecasting + Dynamic Pricing

**Machine Learning + Deep Learning Project**  
**By: _Pranavi Devulapalli_**

---

##  Project Summary

This project builds an **end-to-end demand forecasting system** combined with a **simple dynamic pricing engine**.

It uses:

- **Classical ML** â†’ Random Forest Regressor  
- **Deep Learning** â†’ LSTM (Long Short-Term Memory network)

The project demonstrates:

- âœ… How to forecast daily demand using lag-based features  
- âœ… How to compare classical ML vs deep learning for time-series  
- âœ… How to simulate demand as a function of price  
- âœ… How to estimate optimal price using a revenue curve  
- âœ… How to combine forecasting + pricing into a business pipeline  

---

##  1. Problem Statement

Given daily order data, the goal is to predict:

- Future daily demand  
- The best price to charge to maximize revenue  

This mimics how **e-commerce, food delivery, logistics, and retail platforms** make pricing decisions in the real world.

---

##  2. Dataset

Columns included (may vary slightly by version):

- **Daily order counts** (Target)  
- **Urgent / Non-urgent / Returned orders**  
- **Day-of-week, week-of-month**  
- **Lag features**  
- **Rolling averages**

The dataset does **not** include price, so a **simulated economic priceâ€“demand model** is used in the pricing step.

---

##  3. Feature Engineering (Core of the Project)

Time-series forecasting requires the model to understand **recent history** and **patterns over time**.

The following features were created:

### âœ” Lag Features

- `lag_1` â†’ yesterdayâ€™s demand  
- `lag_7` â†’ demand 1 week ago  

These capture **short and medium-term temporal effects**.

### âœ” Rolling Window Features

- `rolling_mean_7` â†’ average of last 7 days  

This helps:

- Smooth day-to-day noise  
- Capture **trend** and **weekly behavior**

### âœ” Calendar Features

- Day of week  
- Week of month  

These features model **weekly seasonality** (e.g., weekends vs weekdays).

### âœ” Why Feature Engineering Matters

- Tree-based models like **Random Forest** depend heavily on engineered features.  
- **LSTMs** can learn temporal structure internally, but require:
  - Proper scaling  
  - Sufficient data  
  - Sequence formatting

---

## 4. Train/Test Split (Time-based)

- Data was split using **80% past â†’ 20% future**.  
- **No shuffling** was performed.  
- This preserves the natural **time order**, which is critical for time-series forecasting.

---

##  5. Random Forest Model

### âœ” Why Random Forest?

Random Forest works extremely well on:

- Structured **tabular data**  
- **Engineered features** (lags, rolling stats, calendar variables)  
- **Non-linear relationships**  
- **Small to medium-sized datasets**

### âœ” Results

In this project, **Random Forest performed better than LSTM**.

####  Why Random Forest beat LSTM here:

- **Dataset size is small**  
  - LSTMs generally need **large sequential datasets** to shine.

- **Feature engineering was strong**  
  - Lag features + rolling averages already encode temporal structure well.

- **Noise and irregularity**  
  - Tree models like Random Forest handle noisy patterns robustly.

- **LSTM underfit on this dataset**  
  - With limited data, LSTMs tend to output near-constant or smoothed predictions.

- **Random Forest does not require scaling**  
  - LSTM is more sensitive to feature scaling and sequence formatting.

### âœ” Interpretation

Random Forest captured the **non-linear relationships** between lag features, rolling means, and demand **more effectively** than LSTM in this particular setting.

---

##  6. LSTM Model (Deep Learning)

### âœ” Why LSTM?

LSTMs are designed to capture **temporal dependencies** by directly learning from input **sequences**.

### âœ” Sequence Construction

A sequence window of **14 days** was used:

- **Input:** 14-day window of features  
- **Target:** demand on **day 15**  

Each training sample corresponds to:

> â€œGiven the last 14 days, predict demand for the next day.â€

### âœ” Why LSTM performed worse here:

- Dataset was **too small** for deep learning to generalize well.  
- LSTM requires **more historical depth** and data density.  
- Harder to learn strong seasonality without large sequences.  
- LSTM is **sensitive to scaling and hyperparameters**.  
- With limited data, LSTMs often tend to output **â€œflatâ€ or smoothed predictions**.

### âœ” Still Valuable for the Project

Including the LSTM model demonstrates:

- âœ” Knowledge of deep learning architectures  
- âœ” Understanding of **sequence modeling**  
- âœ” Ability to **compare classical ML vs DL** and choose the appropriate one

---

##  7. Dynamic Pricing Engine (Simulated)

Since the dataset does not contain a price column, a **simple price elasticity model** was used to simulate how demand responds to different prices.

### âœ” Priceâ€“Demand Formula

We assume a **linearized price elasticity** around a base price:

\[
d(p) = d_{\text{base}} \times \left( 1 - e \cdot \frac{p - p_{\text{base}}}{p_{\text{base}}} \right)
\]

Where:

- \( d(p) \) = simulated demand at price \( p \)  
- \( d_{\text{base}} \) = predicted demand at base price (from the model)  
- \( p_{\text{base}} = 100 \) â†’ reference/base price  
- \( e \) = elasticity coefficient (e.g., 0.4)  

**Intuition:**

- If \( p > p_{\text{base}} \) â†’ demand decreases  
- If \( p < p_{\text{base}} \) â†’ demand increases  
- Larger \( e \) â†’ more sensitive demand is to price changes

### âœ” Revenue Formula

\[
R(p) = p \times d(p)
\]

### âœ” What the Pricing Engine Does

1. Takes predicted demand (`d_base`) from the ML/DL model.  
2. Simulates demand over a range of candidate prices (e.g., â‚¹60â€“â‚¹140).  
3. Computes **revenue** at each price using `R(p) = p * d(p)`.  
4. Selects the price with **maximum simulated revenue**.

### âœ” Why this is useful

This converts **pure forecasting** into a **business decision tool**.

Companies apply similar ideas in:

- Dynamic **surge pricing** (e.g., Uber)  
- **Hotel room** pricing  
- **Airline** ticket pricing  
- **E-commerce** promotional pricing  

---

## ðŸ“ˆ 8. Visualizations

The notebook includes several helpful visualizations:

### âœ” Distribution of Target

- Helps understand **skewness**, **spread**, and **outliers** in daily demand.

### âœ” Correlation Heatmap

- Reveals relationships between features.  
- Highlights which features are most related to demand.

### âœ” Actual vs Predicted (Random Forest vs LSTM)

- Compares model behavior visually.  
- Shows that Random Forest tracks demand more closely than LSTM.

### âœ” Revenue vs Price Curve

- Plots **Revenue** as a function of **Price**.  
- Shows the approximate **optimal price** where revenue peaks.

---

##  9. Results Summary

| Model          | RMSE      | MAE       | RÂ²        | Notes                       |
|----------------|----------:|----------:|----------:|-----------------------------|
| **Random Forest** | â­ Lower  | â­ Lower  | â­ Higher | Best-performing model       |
| **LSTM**          | Higher   | Higher   | Negative  | Underfit on small dataset   |

> **Key outcome:** On this dataset, **classical ML (Random Forest)** outperformed **deep learning (LSTM)**.

---

##  10. Key Insights

- **Classical ML beat Deep Learning** on this dataset  
  â†’ Important reminder that **DL is not always better**.

- **Lag Features + Rolling Averages are powerful**  
  â†’ Good feature engineering can make traditional models very strong.

- **LSTM requires larger sequential data** to truly shine  
  â†’ Especially for capturing complex seasonality and long-term trends.

- Combining **demand forecasting + pricing** leads to an **end-to-end business solution**, not just a predictive model.

- A **revenue-maximizing price** can be computed by combining demand forecasts with a simple economic model.

---

##  11. Future Work

Potential improvements and extensions:

- Incorporate **real price data** instead of a simulated elasticity model.  
- Add external features:
  - Weather data  
  - Promotions/discount campaigns  
  - Holidays and events  
- Explore **attention-based models** (Transformers for time-series).  
- Perform **systematic hyperparameter tuning** for LSTM.  
- Add classical time-series baselines:
  - **Prophet**  
  - **ARIMA / SARIMA**  

---

##  Author

** Pranavi Devulapalli**  

